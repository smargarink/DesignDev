{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Fourteen: Project Design Starter\n",
    "In this exercise, you'll be planning out a complex project. You'll draw in some code, but focus on commenting to describe your project structure. The sample document below will guide you through organizing and annotating your project design. The primary components you'll include are:\n",
    "\n",
    "- Dependencies: What modules will your project need?\n",
    "- Collection: Where is your data coming from?\n",
    "- Processing: How will you format and process your data?\n",
    "- Analysis: What techniques will you use to understand your data?\n",
    "- Visualization: How will you visualize and explore your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "I plan on extracting information from Twitter using #BlerDCon. BlerDcon is a convention — the event’s name signals that it specifically caters to blerds (black nerds), BlerDCon is an inclusive space for all POC, LGBTQ+, female, and disabled fans. Pulling data from the 2021 convention, I seek to gather more information and insight into the topics dicussed around this venue and the sentiments of the participants. \n",
    "\n",
    "At first, I thought I could pull all the tweets and analyze keywords but ran into some issues, tried to create a HTML file and pull words from there but formatting code causes to many issues. \n",
    "\n",
    "Trued editing our location paramaters to make data set larger but data set remains small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "Importing modules to extract data from hastags on twitter, compile that information into a digestible list, and then use that info to create visualizations for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snscrape.modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q1/vbgdcffj4wqf34256n43wzz40000gn/T/ipykernel_12573/2517099647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#trying a new scraper since Tweepy only pulls recent tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snscrape.modules'"
     ]
    }
   ],
   "source": [
    "# Importing Config to pull credentials for Twitter API\n",
    "import configparser\n",
    "\n",
    "#Importing Twitter API\n",
    "import tweepy\n",
    "\n",
    "#To turn data into a CSV file\n",
    "import csv\n",
    "\n",
    "#Importing to help extract data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Importing to map out geographical locations of responses\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "#importing to plot findings in visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing for map visualization\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.patches import Circle\n",
    "\n",
    "#importing to assist in compliling \n",
    "import re\n",
    "\n",
    "#Importing for punctuation\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "#importing to make a wordcloud for visualizations\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "#To turn work cloud into an image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "#Importing to gather sentiments from comments\n",
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Importing Pandas to handle collected Twitter data \n",
    "import pandas as pd\n",
    "\n",
    "#trying a new scraper since Tweepy only pulls recent tweets\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection\n",
    "Collecting data from recent 2021 convention hastag #BlerDcon. Starting date from pull will begin 01-01-2021 to current time. This can be changed later to look into other year conventions. \n",
    "\n",
    "Tried to change to #BlerDCon to widen paramaters. \n",
    "\n",
    "I checked other avenues of collecting \"customer feedback\" but Twitter seems to contain the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling credentials file, so Twiiter Developer account can be accessed\n",
    "CONFIG = configparser.ConfigParser()\n",
    "CONFIG.read('credentials.ini')\n",
    "\n",
    "#Tweepy\n",
    "auth = tweepy.OAuthHandler(CONFIG['DEFAULT']['consumer_key'], CONFIG['DEFAULT']['consumer_secret'])\n",
    "auth.set_access_token(CONFIG['DEFAULT']['access_token'], CONFIG['DEFAULT']['access_token_secret'])\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    #dont need to see everything\n",
    "    print(tweet.text [0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling info in from specific hashtag\n",
    "#Need to figure out how to accurately pull all the tweets with specific hasgtag and compile it into a CSV file for analysis.\n",
    "#Started with BlerDCon2021 and then just BlerdCon\n",
    "search_words = \"#BlerDCon\"\n",
    "date_since = \"2010-01-01\"\n",
    "tweets = tweepy.Cursor(api.search_tweets,\n",
    "              q=search_words, lang=\"en\").items(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "\n",
    "keyword = 'BlerDCon2021'\n",
    "maxTweets = 30000\n",
    "\n",
    "#Open/create a file to append data to\n",
    "csvFile = open('BlerDCon2021.csv', 'a', newline='', encoding='utf8')\n",
    "\n",
    "#Use csv writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "csvWriter.writerow(['id','date','tweet'])\n",
    "\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' since:2021-06-01 until:2021-12-16').get_items()) :\n",
    "        if i > maxTweets :\n",
    "            break\n",
    "        csvWriter.writerow([tweet.id, tweet.date, tweet.renderedContent])\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Trying another way around Twitter. Found Youtube video discussing topic -- will pull comments from there.\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "\n",
    "url = 'https://www.lipstickalley.com/threads/blerdcon-a-white-girl-wins-the-cosplay-event-is-that-ok.4605659/'\n",
    "\n",
    "response = urllib.request.urlopen(url)\n",
    "HTML = response.read()\n",
    "\n",
    "print(HTML[0: 2000])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''contents = str(HTML)\n",
    "startLoc = contents.find(\"comments\")\n",
    "endLoc = contents.find(\"<div class='answererUserIcon'>\")\n",
    "\n",
    "print(startLoc)\n",
    "print(endLoc)\n",
    "\n",
    "contents = contents[startLoc: endLoc]\n",
    "print(contents[0: 2000])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "After your data has been collected or imported, store it in a format that works for your purposes. This can vary: for Twitter analysis, it might be a Pandas dataframe, while for text, you might build a document term matrix.\n",
    "\n",
    "I seem to be successful at pulling some locations but not all tweets. I know the dataset is large cause i've checked it in Twitter. Tried a few exmaples from online but could not get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to sort tweets\n",
    "#pulled location out of script to see if username and tweet could just be gathered - this did not worl\n",
    "tweets_sorted = [[tweet.user.screen_name,tweet.text] for tweet in tweets]\n",
    "\n",
    "#caluculating coordinates of Tweets -- doesnt look like too many locations (may not use for data set)\n",
    "#tried removing location to just get tweets\n",
    "tdf = pd.DataFrame(data=tweets_sorted, columns=['user', 'tweet'])\n",
    "print(tdf)\n",
    "\n",
    "#may need to look for word count instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can create the file but not add the contents of my hastag scrape to it.\n",
    "# Ideally a CSV will exist with username, location, and tweet.\n",
    "csvFile = open('BlerDCon', 'a')\n",
    "csvWriter = csv.writer(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''tdf = pd.DataFrame(output)\n",
    "tdf.to_csv('output.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#need to organize CSV better then this function will work better\n",
    "\n",
    "tdf = pd.read_csv('output.csv')\n",
    "# Prints how pd will structure data types\n",
    "print(df.dtypes)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#trying to read CSV another way\n",
    "f = open('output.csv', 'r')\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "#split_string\n",
    "word_bag = text.split()\n",
    "print(word_bag[0:100])\n",
    "\n",
    "#make lower case\n",
    "text = text.lower()\n",
    "word_bag = text.split()\n",
    "print(word_bag[0:100])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "Would like to analyze the CSV file for common key terms and number of times a topic is mentioned. From their try to build out sentiments of users. \n",
    "\n",
    "Seemed to only work with location. Research should focus on common topics and sentiments around the convention -- would have led to controversy from 2021 convention where a white woman won best cosplay at a black convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this didnt matter cause there was only one location\n",
    "locs = tdf['location'].value_counts()\n",
    "print(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "geolocator = Nominatim(user_agent='twitter-analysis-client')\n",
    "limited = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "\n",
    "def find_location(row):\n",
    "    place = row['location']\n",
    "    location = limited(place)\n",
    "    \n",
    "    if location != None:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return \"Not Found\", \"Not Found\"\n",
    "\n",
    "tdf[['latitude','longitude']] = tdf.apply(find_location, axis=\"columns\", result_type=\"expand\")\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams['figure.figsize'] = (20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "The goal was to create a word cloud with common topics from the hashtag and place it over the logo for the conference. A Bokeh chart would be helpful here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = tdf.groupby(['latitude','longitude']).size().reset_index().rename(columns={0:'count'})\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.stock_img()\n",
    "# plot individual locations                                                                                                       \n",
    "for i in range(0, len(lats)):\n",
    "    ax.plot(longs[i], lats[i], 'ro', transform=ccrs.PlateCarree())\n",
    "                           \n",
    "# add coastlines for reference                                                                                                \n",
    "ax.coastlines(resolution='50m')\n",
    "ax.set_global()\n",
    "def get_radius(freq):\n",
    "    if freq < 5:\n",
    "        return 1\n",
    "    elif freq < 10:\n",
    "        return 2\n",
    "    elif freq >= 10:\n",
    "        return 3\n",
    "# plot count of tweets per location\n",
    "\n",
    "colLats = counter['latitude']\n",
    "colLongs = counter['longitude']\n",
    "colCounts = counter['count']\n",
    "\n",
    "for i in range(0, len(counter)):\n",
    "    ax.add_patch(Circle(xy=[colLongs[i], colLats[i]], radius=get_radius(colCounts[i]), color='blue',alpha=0.5, transform=ccrs.PlateCarree()))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "260556d7196b2a521c66178a7d32a40a7875a519cf758c158bc134912987d078"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
