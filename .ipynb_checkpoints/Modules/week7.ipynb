{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "corpus = [\"I am an invisible man\",\n",
    "          \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move\",\n",
    "          \"Mother died today. Or maybe, yesterday; I can't be sure\",\n",
    "          \"It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\",\n",
    "          \"Ships at a distance have every man’s wish on board\",\n",
    "          \"We were somewhere around Barstow on the edge of the desert when the drugs began to take hold\",\n",
    "          \"It was a bright cold day in April, and the clocks were striking thirteen\",\n",
    "          \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed in his bed into a gigantic insect\"]\n",
    "\n",
    "document = corpus[2]\n",
    "print(document.split())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Mother', 'died', 'today.', 'Or', 'maybe,', 'yesterday;', 'I', \"can't\", 'be', 'sure']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# %%\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "document = corpus[3]\n",
    "print(nltk.tokenize.word_tokenize(document, language='english'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['It', 'was', 'a', 'queer', ',', 'sultry', 'summer', ',', 'the', 'summer', 'they', 'electrocuted', 'the', 'Rosenbergs', ',', 'and', 'I', 'didn', '’', 't', 'know', 'what', 'I', 'was', 'doing', 'in', 'New', 'York']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#we will only take sentances without certain puncuation marks\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "\n",
    "\n",
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "\n",
    "    Arguments:\n",
    "        string (str): a string to check for punctuation markers.\n",
    "\n",
    "    Returns:\n",
    "        bool: True is string is a (sequence of) punctuation marker(s),\n",
    "            False otherwise.\n",
    "\n",
    "    Examples:\n",
    "        >>> is_punct(\"!\")\n",
    "        True\n",
    "        >>> is_punct(\"Bonjour!\")\n",
    "        False\n",
    "        >>> is_punct(\"¿Te gusta el verano?\")\n",
    "        False\n",
    "        >>> is_punct(\"...\")\n",
    "        True\n",
    "        >>> is_punct(\"«»...\")\n",
    "        True\n",
    "\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokens = nltk.tokenize.word_tokenize(corpus[2], language='english')\n",
    "\n",
    "# Loop with a standard for-loop\n",
    "tokenized = []\n",
    "for token in tokens:\n",
    "    if not is_punct(token):\n",
    "        tokenized.append(token)\n",
    "print(tokenized)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Mother', 'died', 'today', 'Or', 'maybe', 'yesterday', 'I', 'ca', \"n't\", 'be', 'sure']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    Arguments:\n",
    "        text (str): a string representing a text.\n",
    "        language (str): a string specifying the language of text.\n",
    "        lowercase (bool, optional): Set to True to lowercase all\n",
    "            word tokens. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list: a list of word tokens extracted from text, excluding\n",
    "            punctuation.\n",
    "\n",
    "    Examples:\n",
    "        >>> preprocess_text(\"Ah! Monsieur, c'est donc vous?\", 'french')\n",
    "        [\"ah\", \"monsieur\", \"c'est\", \"donc\", \"vous\"]\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "for document in corpus[2:4]:\n",
    "    print('Original:', document)\n",
    "    print('Tokenized:', preprocess_text(document, 'english'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original: Mother died today. Or maybe, yesterday; I can't be sure\n",
      "Tokenized: ['mother', 'died', 'today', 'or', 'maybe', 'yesterday', 'i', 'ca', \"n't\", 'be', 'sure']\n",
      "Original: It was a queer, sultry summer, the summer they electrocuted the Rosenbergs, and I didn’t know what I was doing in New York\n",
      "Tokenized: ['it', 'was', 'a', 'queer', 'sultry', 'summer', 'the', 'summer', 'they', 'electrocuted', 'the', 'rosenbergs', 'and', 'i', 'didn', 't', 'know', 'what', 'i', 'was', 'doing', 'in', 'new', 'york']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import collections\n",
    "\n",
    "vocabulary = collections.Counter()\n",
    "for document in corpus:\n",
    "    vocabulary.update(preprocess_text(document, 'english'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(vocabulary.most_common(n=5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 9), ('a', 6), ('i', 4), ('in', 4), ('was', 4)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print('Original vocabulary size:', len(vocabulary))\n",
    "pruned_vocabulary = {token for token, count in vocabulary.items() if count > 1}\n",
    "print(pruned_vocabulary)\n",
    "print('Pruned vocabulary size:', len(pruned_vocabulary))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original vocabulary size: 100\n",
      "{'in', 'man', 'i', 'the', 'was', 'and', 'summer', 'on', 'of', 'it', 'were', 'as', 'a'}\n",
      "Pruned vocabulary size: 13\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\n",
    "            of lists of strings.\n",
    "        min_count (int, optional): the minimum occurrence count of a\n",
    "            vocabulary item in the corpus.\n",
    "        max_count (int, optional): the maximum occurrence count of a\n",
    "            vocabulary item in the corpus. Defaults to inf.\n",
    "\n",
    "    Returns:\n",
    "        list: An alphabetically ordered list of unique words in the\n",
    "            corpus, of which the frequencies adhere to the specified\n",
    "            minimum and maximum count.\n",
    "\n",
    "    Examples:\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\n",
    "                      ['a', 'story', 'book', 'drama']]\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\n",
    "        ['book', 'drama', 'love', 'man', 'the']\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenized_corpus = [preprocess_text(document, 'english') for document in corpus]\n",
    "vocabulary = extract_vocabulary(tokenized_corpus)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "bags_of_words = []\n",
    "for document in tokenized_corpus:\n",
    "    tokens = [word for word in document if word in vocabulary]\n",
    "    bags_of_words.append(collections.Counter(tokens))\n",
    "\n",
    "print(bags_of_words[5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'the': 3, 'we': 1, 'were': 1, 'somewhere': 1, 'around': 1, 'barstow': 1, 'on': 1, 'edge': 1, 'of': 1, 'desert': 1, 'when': 1, 'drugs': 1, 'began': 1, 'to': 1, 'take': 1, 'hold': 1})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Novel Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "# download the most recent punkt package\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "corpus = [\"alice.txt\",\"anne.txt\",\"oz.txt\"]\n",
    "titles = [\"Alice in Wonderland\", \"Anne of Green Gables\", \"Wizard of Oz\"]\n",
    "documents = []\n",
    "for url in corpus:\n",
    "    f = open(url, encoding='utf-8')\n",
    "    text = f.read()\n",
    "    documents.append(text)\n",
    "print(documents[1][0:100])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'alice.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q1/vbgdcffj4wqf34256n43wzz40000gn/T/ipykernel_75959/3364169266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'alice.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\n",
    "\n",
    "\n",
    "def is_punct(string):\n",
    "    \"\"\"Check if STRING is a punctuation marker or a sequence of\n",
    "       punctuation markers.\n",
    "    \"\"\"\n",
    "    return PUNCT_RE.match(string) is not None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_text(text, language, lowercase=True):\n",
    "    \"\"\"Preprocess a text.\n",
    "\n",
    "    Perform a text preprocessing procedure, which transforms a string\n",
    "    object into a list of word tokens without punctuation markers.\n",
    "\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenized = []\n",
    "for text in documents:\n",
    "    tokenized.append(preprocess_text(text, \"english\"))\n",
    "\n",
    "print(tokenized[0][11])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\n",
    "    \"\"\"Extract a vocabulary from a tokenized corpus.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus represented, list\n",
    "            of lists of strings.\n",
    "        min_count (int, optional): the minimum occurrence count of a\n",
    "            vocabulary item in the corpus.\n",
    "        max_count (int, optional): the maximum occurrence count of a\n",
    "            vocabulary item in the corpus. Defaults to inf.\n",
    "\n",
    "    Returns:\n",
    "        list: An alphabetically ordered list of unique words in the\n",
    "            corpus, of which the frequencies adhere to the specified\n",
    "            minimum and maximum count.\n",
    "\n",
    "    Examples:\n",
    "        >>> corpus = [['the', 'man', 'love', 'man', 'the'],\n",
    "                      ['the', 'love', 'book', 'wise', 'drama'],\n",
    "                      ['a', 'story', 'book', 'drama']]\n",
    "        >>> extract_vocabulary(corpus, min_count=2)\n",
    "        ['book', 'drama', 'love', 'man', 'the']\n",
    "\n",
    "    \"\"\"\n",
    "    vocabulary = collections.Counter()\n",
    "    for document in tokenized_corpus:\n",
    "        vocabulary.update(document)\n",
    "    vocabulary = {word for word, count in vocabulary.items()\n",
    "                  if count >= min_count and count <= max_count}\n",
    "    return sorted(vocabulary)\n",
    "\n",
    "#dont show any words that appear less than min count\n",
    "import collections\n",
    "vocabulary = extract_vocabulary(tokenized, min_count=2)\n",
    "print(vocabulary[0:100])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def corpus2dtm(tokenized_corpus, vocabulary):\n",
    "    \"\"\"Transform a tokenized corpus into a document-term matrix.\n",
    "\n",
    "    Arguments:\n",
    "        tokenized_corpus (list): a tokenized corpus as a list of\n",
    "        lists of strings. vocabulary (list): An list of unique words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists representing the frequency of each term\n",
    "              in `vocabulary` for each document in the corpus.\n",
    "\n",
    "    Examples:\n",
    "        >>> tokenized_corpus = [['the', 'man', 'man', 'smart'],\n",
    "                                ['a', 'the', 'man' 'love'],\n",
    "                                ['love', 'book', 'journey']]\n",
    "        >>> vocab = ['book', 'journey', 'man', 'love']\n",
    "        >>> corpus2dtm(tokenized_corpus, vocabulary)\n",
    "        [[0, 0, 2, 0], [0, 0, 1, 1], [1, 1, 0, 1]]\n",
    "\n",
    "    \"\"\"\n",
    "    document_term_matrix = []\n",
    "    for document in tokenized_corpus:\n",
    "        document_counts = collections.Counter(document)\n",
    "        row = [document_counts[word] for word in vocabulary]\n",
    "        document_term_matrix.append(row)\n",
    "    return document_term_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "document_term_matrix = np.array(corpus2dtm(tokenized, vocabulary))\n",
    "girl_id = vocabulary.index('girl')\n",
    "boy_id = vocabulary.index('boy')\n",
    "\n",
    "girl_counts = document_term_matrix[:, girl_id]\n",
    "boy_counts = document_term_matrix[:, boy_id]\n",
    "print(\"Girl: \" + str(girl_counts))\n",
    "print(\"Boy: \" + str(boy_counts))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(titles))\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, girl_counts, width, label='Girl')\n",
    "rects2 = ax.bar(x + width/2, boy_counts, width, label='Boy')\n",
    "\n",
    "ax.set_ylabel('Word Count')\n",
    "ax.set_title('Gender Binary Word Frequency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(titles)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alice = np.array([girl_counts[0], boy_counts[0]])\n",
    "anne = np.array([girl_counts[1], boy_counts[1]])\n",
    "oz = np.array([girl_counts[2], boy_counts[2]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Compute the Euclidean distance between two vectors.\n",
    "\n",
    "    Note: ``numpy.linalg.norm(a - b)`` performs the\n",
    "    same calculation using a slightly faster method.\n",
    "\n",
    "    Arguments:\n",
    "        a (numpy.ndarray): a vector of floats or ints.\n",
    "        b (numpy.ndarray): a vector of floats or ints.\n",
    "\n",
    "    Returns:\n",
    "        float: The euclidean distance between vector a and b.\n",
    "\n",
    "    Examples:\n",
    "        >>> import numpy as np\n",
    "        >>> a = np.array([1, 4, 2, 8])\n",
    "        >>> b = np.array([2, 1, 4, 7])\n",
    "        >>> round(euclidean_distance(a, b), 2)\n",
    "        3.87\n",
    "\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "aliceanne = euclidean_distance(alice, anne)\n",
    "print(f'Alice - Anne: {aliceanne:.2f}')\n",
    "\n",
    "aliceoz = euclidean_distance(alice, oz)\n",
    "print(f'Alice - Oz: {aliceoz:.2f}')\n",
    "\n",
    "anneoz = euclidean_distance(anne, oz)\n",
    "print(f'Anne - Oz: {anneoz:.2f}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "a3bce285177441cdc767e3a9c4ffb6c946df65ec3625de136fb9d1815a6425f2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}